{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicks165/VectorDatabases/blob/main/Weaviate_evaluation_cohere_wiki_english_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN3yoTBPg1-v"
      },
      "source": [
        "https://weaviate.io/developers/weaviate/quickstart/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnAy6uxX7wJe"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client cohere datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX2e18c5fcxs"
      },
      "source": [
        "Initiatize Weaviate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBY861sa_8tV"
      },
      "outputs": [],
      "source": [
        "def instantiateWeaviateClient():\n",
        "\n",
        "  auth_config = weaviate.AuthApiKey(api_key=\"ODYjhwMLhyrzL3daMEx7PsKna3AtBDJZU98J\")\n",
        "  client = weaviate.Client(\n",
        "    url=\"https://osv1ykq1soaj5kb6lb1uq.gcp-d.weaviate.cloud\",\n",
        "    auth_client_secret=auth_config\n",
        "  )\n",
        "\n",
        "  return client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laUwLmhamrzD"
      },
      "source": [
        "Initialize index/collection (class in weaviate)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import string\n",
        "import random\n",
        "import json\n",
        "import weaviate\n",
        "import cohere\n",
        "import numpy\n",
        "\n",
        "def initialize_class(client):\n",
        "  # Class definition object. Weaviate's autoschema feature will infer properties when importing.\n",
        "  class_obj = {\n",
        "      \"class\": \"wikipedia_articles\",\n",
        "      \"vectorizer\": \"none\",\n",
        "      \"vectorIndexConfig\": {\n",
        "          \"distance\": \"cosine\"\n",
        "      }\n",
        "  }\n",
        "\n",
        "  #client = instantiateWeaviateClient()\n",
        "  #client.schema.delete_class(class_obj[\"class\"])\n",
        "\n",
        "  # Add the class to the schema\n",
        "  #IMPORTANT = Only execute once per cloud account\n",
        "  if(client.schema.exists(class_obj[\"class\"])):\n",
        "    print(\"Class exists\")\n",
        "  else:\n",
        "    client.schema.create_class(class_obj)\n",
        "\n",
        "\n",
        "  #client.schema.exists(class_obj[\"class\"])\n",
        "  return class_obj"
      ],
      "metadata": {
        "id": "SqOmMLiUSaBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFscTunmoBVj"
      },
      "source": [
        "Confirm shape and save the dataset to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpM5a8kMnkHz"
      },
      "source": [
        "Load embeddings and metadata into Weaviate -\n",
        " 1. Load in batch\n",
        " 2. After hitting a threshold, insert 1 object and measure time\n",
        " 3. Measure query performance for the same threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYa1Wpto_4VK"
      },
      "outputs": [],
      "source": [
        "def upsert_one_record(client, co, class_obj):\n",
        "  # initializing size of string\n",
        "  N = 7\n",
        "\n",
        "  # using random.choices()\n",
        "  # generating random strings\n",
        "  randomString = ''.join(random.choices(string.ascii_uppercase +\n",
        "                             string.digits, k=N))\n",
        "\n",
        "  dataToupd = \"This is a new record \" + randomString\n",
        "\n",
        "  newVector = {\n",
        "      \"vector\": co.embed(texts=[dataToupd], model='multilingual-22-12').embeddings[0]\n",
        "  }\n",
        "\n",
        "  # Configure a batch process\n",
        "  # create metadata dictionary\n",
        "  properties = {\n",
        "          \"text\": \"New Data added \" + dataToupd,\n",
        "          }\n",
        "\n",
        "  start_time = time.time()\n",
        "  #client.batch.add_data_object(properties, class_obj[\"class\"], vector=newVector[\"vector\"])\n",
        "\n",
        "  data_uuid = client.data_object.create(\n",
        "    properties,\n",
        "    class_obj[\"class\"],\n",
        "    vector = newVector[\"vector\"]\n",
        "  )\n",
        "\n",
        "  print(\"Updated with one Record (uuid: {0}) and Time taken --- {1} seconds ---\".format(data_uuid, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weuGgAecrRGt"
      },
      "source": [
        "Querying the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwbN5W5hrWTs"
      },
      "outputs": [],
      "source": [
        "def issue_measure_query_time(client, co, class_obj):\n",
        "\n",
        "  query1 = \"What was the cause of the major recession in the early 20th century?\"\n",
        "  query2 = \"Where is Mount Everest?\"\n",
        "  query3 = \"something else\"\n",
        "\n",
        "  queries = [query1, query2, query3]\n",
        "\n",
        "  timeTakenList = []\n",
        "\n",
        "  ## Issue 3 queries and take the average\n",
        "\n",
        "  for i in range(0, 2):\n",
        "    # create the query embedding\n",
        "    nearVector = {\n",
        "        \"vector\": co.embed(texts=queries, model='multilingual-22-12').embeddings[i]\n",
        "    }\n",
        "\n",
        "    query_start_time = time.time()\n",
        "\n",
        "    result = client.query.get(class_obj[\"class\"], [\"text\"]).with_near_vector(nearVector).with_limit(2).with_additional(['certainty']).do()\n",
        "\n",
        "    query_end_time = time.time()\n",
        "\n",
        "    timeTakenList.append(query_end_time - query_start_time)\n",
        "\n",
        "    #print(\" For query number {0}, time taken for search = {1} \".format(queries[i], timeTakenList[i]))\n",
        "\n",
        "   # print(json.dumps(result, indent=4))\n",
        "\n",
        "  averageTimeTaken = numpy.average(timeTakenList)\n",
        "  print(\"Average time taken for search = {0} \".format(averageTimeTaken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzp2810LOrRO"
      },
      "source": [
        "conditional filtering on search query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf5htwHfOqoK"
      },
      "outputs": [],
      "source": [
        "def conditional_search(client, co, class_obj):\n",
        "\n",
        "  query1 = \"What was the cause of the major recession in the early 20th century?\"\n",
        "  query2 = \"Where is Mount Everest?\"\n",
        "  query3 = \"something else\"\n",
        "\n",
        "  queries = [query1, query2, query3]\n",
        "\n",
        "  timeTakenList = []\n",
        "\n",
        "  ## Issue 3 queries and take the average\n",
        "\n",
        "  for i in range(0, 2):\n",
        "    # create the query embedding\n",
        "    nearVector = {\n",
        "        \"vector\": co.embed(texts=queries, model='multilingual-22-12').embeddings[i]\n",
        "    }\n",
        "\n",
        "    query_start_time = time.time()\n",
        "\n",
        "    result = (client.query\n",
        "              .get(class_obj[\"class\"], [\"text\"])\n",
        "              .with_near_vector(nearVector)\n",
        "              .with_where({\n",
        "                \"path\": [\"text\"],\n",
        "                \"operator\": \"Like\",\n",
        "                \"valueText\": \"recession\"\n",
        "                })\n",
        "              .with_limit(2)\n",
        "              .with_additional(['certainty'])\n",
        "              .do())\n",
        "\n",
        "    query_end_time = time.time()\n",
        "\n",
        "    timeTakenList.append(query_end_time - query_start_time)\n",
        "\n",
        "    #print(\" For query number {0}, time taken for conditional search = {1} \".format(queries[i], timeTakenList[i]))\n",
        "\n",
        "    #print(json.dumps(result, indent=4))\n",
        "\n",
        "  averageTimeTaken = numpy.average(timeTakenList)\n",
        "  print(\"Average time taken for conditional search = {0} \".format(averageTimeTaken))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import uuid\n",
        "MAX_ENTRIES = 35000000\n",
        "\n",
        "def upsert_db_measure(docs, doc_embeddings, client, batch_size, total_inserted, workload_start_time, class_obj, previous_run_time):\n",
        "\n",
        "  co = cohere.Client(f\"o7lTEJeC1QHjU5I4Ee6U2I0m6l5wCOUPWqwoGM7H\")  # Add your cohere API key from www.cohere.com\n",
        "\n",
        "  # Configure a batch process, calls flush at the end of with block implicitly\n",
        "  with client.batch as batch:\n",
        "    batch.batch_size=batch_size\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Batch import all Questions\n",
        "    for i, doc in enumerate(docs):\n",
        "      # create metadata dictionary\n",
        "      properties = {\n",
        "          \"text\": doc[\"text\"],\n",
        "      }\n",
        "\n",
        "      client.batch.add_data_object(properties, class_obj[\"class\"], vector=doc_embeddings[i])\n",
        "\n",
        "  total_inserted += batch_size\n",
        "\n",
        "  if(total_inserted in range(0, MAX_ENTRIES, 100000)):\n",
        "    print(\"=======================================================================================================\")\n",
        "    total_time = (time.time() - workload_start_time) + previous_run_time\n",
        "    print(\"For {0} entries, time taken for inserts = {1} \".format(total_inserted, total_time))\n",
        "    upsert_one_record(client, co, class_obj)\n",
        "    issue_measure_query_time(client, co, class_obj)\n",
        "    conditional_search(client, co, class_obj)\n",
        "\n",
        "  return total_inserted"
      ],
      "metadata": {
        "id": "YsSs5o0HRNXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load datset and get the embeddings from Cohere dataset at Huggingface"
      ],
      "metadata": {
        "id": "RPPOReED0FBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import cohere\n",
        "\n",
        "# Create a generator that yields chunks of the dataset\n",
        "def chunk_generator(dataset, chunk_size, starting_index):\n",
        "  for i in range(starting_index, len(dataset), chunk_size):\n",
        "    yield dataset[i:i + chunk_size]\n",
        "\n",
        "def load_cohere_dataset():\n",
        "   # bring dataset to disk in Arrow table format\n",
        "  dataset = load_dataset(f\"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\")\n",
        "  return dataset\n",
        "\n",
        "def load_execute_workload(client, class_obj, dataset):\n",
        "\n",
        "  limit = -1 # keep -1 for all, else update to a positive number to limit\n",
        "\n",
        "  chunk_size = 1000 # size of batch upserts and items kept in memory\n",
        "\n",
        "  max_docs_loaded = 0\n",
        "\n",
        "  #if the runs fails, we want to re-start and for the subsequent time measurements to be valid. Use this variable\n",
        "  # paste the runtime for previous starting_index or set it to 0 otherwise\n",
        "  previous_run_time = 124516.61101400756 # set to 0 when starting from scratch\n",
        "  previous_docs_loaded = 30500000 # set to 0 when starting from scratch\n",
        "  max_docs_loaded = previous_docs_loaded\n",
        "\n",
        "  start_time = time.time()\n",
        "  docs = []\n",
        "  doc_embeddings = []\n",
        "  # Iterate over the chunks\n",
        "  for chunk in chunk_generator(dataset, chunk_size, previous_docs_loaded):\n",
        "    for i in range(0, chunk_size):\n",
        "      docs.append({\"text\" : chunk[\"text\"][i]})\n",
        "      doc_embeddings.append(chunk['emb'][i])\n",
        "\n",
        "    max_docs_loaded = upsert_db_measure(docs, doc_embeddings, client, chunk_size, max_docs_loaded, start_time, class_obj, previous_run_time)\n",
        "\n",
        "    # clear the lists because we want to re-use them for the next chunk\n",
        "    docs.clear()\n",
        "    doc_embeddings.clear()\n",
        "\n",
        "    if (limit > 0 and max_docs_loaded >= limit):\n",
        "      break\n",
        "\n",
        "  total_time = (time.time() - start_time) + previous_run_time\n",
        "  print (\"succesfully executed workload for {0} entries with total time {1}\"\n",
        "    .format(max_docs_loaded, total_time))"
      ],
      "metadata": {
        "id": "9rsehvWkQKHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a big dataset is expensive. Seperate this step so that executing and debugging the main functions would be simple"
      ],
      "metadata": {
        "id": "HKImiHTFkk_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_cohere_dataset()"
      ],
      "metadata": {
        "id": "prhgy1GIkgda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperating from main execution."
      ],
      "metadata": {
        "id": "_F28RyCcEgZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#client = instantiateWeaviateClient()\n",
        "#class_obj = initialize_class(client)"
      ],
      "metadata": {
        "id": "ur-N9ojrEe53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = instantiateWeaviateClient()\n",
        "class_obj = initialize_class(client)\n",
        "#download dataset and execute workload\n",
        "load_execute_workload(client, class_obj, dataset)"
      ],
      "metadata": {
        "id": "-mbC3gE60MwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}